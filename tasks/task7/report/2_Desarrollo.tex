\textcolor{blue}{\section{The approach of the Sorting Algorithms}}

In the realm of computer science, sorting algorithms are fundamental constructs that order elements in a specific sequence or manner. Each sorting algorithm has a unique design and approach that offers advantages in certain scenarios over others. In this section section we discuss the logic behind three classic sorting algorithms in this research: QuickSort, BubbleSort, and MergeSort, highlighting their complexities and the specific cases where they are most effective.

\subsection{QuickSort}
QuickSort is a divide-and-conquer sorting algorithm that selects an element, referred to as the "pivot", and partitions the array around the pivot. The algorithm then recursively applies the same logic to the sub-arrays. The implementation of the algorithm is represented in Listing \ref{lst:quicksort},

\textbf{Complexity:}
- Average Time Complexity: \(O(n \log n)\)
- Worst-Case Time Complexity: \(O(n^2)\)
- Space Complexity: \(O(\log n)\)

\textbf{Effective Usage:}
QuickSort is usually faster in practice than other \(O(n \log n)\) algorithms like MergeSort or HeapSort. It works especially well when the pivots chosen are median or near-median values. However, its worst-case time complexity can be a limitation with certain datasets or if the pivot selection strategy isn't effective.


\begin{lstlisting}[language=PseudoCode, label=lst:quicksort, caption=QuickSort and its Partition function]
/**
 * Sorts an array using the QuickSort algorithm.
 * 
 * @param A: The array to be sorted.
 * @param start: The starting index of the portion of A to be sorted.
 * @param end: The ending index of the portion of A to be sorted.
 */
Function QuickSort(A, start, end)
    if start < end
        pivotIndex (*@$\leftarrow$@*) Partition(A, start, end)
        QuickSort(A, start, pivotIndex - 1)
        QuickSort(A, pivotIndex + 1, end)
    end if

/**
 * Partitions an array around a pivot, such that elements less than the pivot are
 * on the left, and elements greater than the pivot are on the right.
 * 
 * @param A: The array to be partitioned.
 * @param start: The starting index for the partition.
 * @param end: The ending index for the partition.
 * @return: Index of the pivot after partitioning.
 */
Function Partition(A, start, end)
    pivot (*@$\leftarrow$@*)  A[end]
    pivotIndex (*@$\leftarrow$@*)  start
    for i from start to end - 1
        if A[i] (*@$\leq$@*)  pivot
            swap A[i] and A[pivotIndex]
            pivotIndex (*@$\leftarrow$@*) pivotIndex + 1
        end if
    end for
    swap A[pivotIndex] and A[end]
    return pivotIndex
\end{lstlisting}

\subsection{Average Case Analysis}
The main value of the final complexity depends on the \textbf{Partition} function implementation.
When QuickSort partitions the array evenly, the recurrence relation is given by:
\[
T_{\text{QuickSort}}(n) = 2T_{\text{QuickSort}}\left(\frac{n}{2}\right) + n
\]
Using the Master Theorem, this recurrence yields a time complexity of:
\[
T_{\text{QuickSort}}(n) = O(n \log n)
\]

\subsection{Worst Case Analysis}
For the worst-case scenario, if the pivot is always the smallest or largest element, the partitioning will be highly unbalanced. The recurrence relation becomes:
\[
T_{\text{QuickSort}}(n) = T_{\text{QuickSort}}(n-1) + n
\]
Expanding this recurrence:
\begin{align*}
T_{\text{QuickSort}}(n) & = n + T_{\text{QuickSort}}(n-1) \\
& = n + (n-1) + T(n-2) \\
& = n + (n-1) + (n-2) + \dots + 2 + 1 \\
& = \sum_{i=1}^{n} i \\
& = \frac{n(n+1)}{2} \\
& = \frac{n^2}{2} + \frac{n}{2}
\end{align*}
Thus, in the worst-case, the time complexity is:
\[
T_{\text{QuickSort}}(n) = O(n^2)
\]

\subsection{BubbleSort}

BubbleSort is a simple comparison-based sorting algorithm. It repeatedly steps through the list, compares adjacent elements, and swaps them if they are in the wrong order. This process repeats for each item in the list until the list is sorted.


\textbf{Complexity:}
- Average Time Complexity: \(O(n^2)\)
- Worst-Case Time Complexity: \(O(n^2)\)
- Space Complexity: \(O(1)\)

\textbf{Effective Usage:}
Due to its simplicity, BubbleSort can be effective for smaller lists or lists that are mostly sorted. However, for larger datasets, its quadratic time complexity makes it less efficient compared to more advanced sorting algorithms.  The implementation of the algorithm is represented in Listing \ref{lst:bubblesort},


\begin{lstlisting}[language=PseudoCode, label=lst:bubblesort, caption=BubbleSort Algorithm]
/**
 * Sorts an array using the BubbleSort algorithm.
 * 
 * @param A: The array to be sorted.
 */
Function BubbleSort(A)
    n (*@$\leftarrow$@*)  length(A)
    for i from 0 to n-1
        for j from 0 to n-i-1
            if A[j] > A[j+1]
                swap A[j] and A[j+1]
            end if
        end for
    end for
\end{lstlisting}

\subsection{Worst Case Analysis of BubbleSort}
In the worst-case scenario for BubbleSort, the list is in reverse order. During each iteration of the inner loop, the largest element (or the next largest) is "bubbled up" to its correct position. For an array of size \( n \):

The inner loop will run for:
\[
(n-1) + (n-2) + (n-3) + \dots + 1 = \frac{n(n-1)}{2}
\]
This yields a worst-case time complexity of:
\[
T_{\text{BubbleSort}}(n) = O(n^2)
\]

\subsection{Average Case Analysis of BubbleSort}
For the average case, assuming random input, half of the elements will be out of order on average. This means that the inner loop will still perform approximately half of its maximum number of comparisons, which results in the quadratic time complexity.

Thus, the average case time complexity is also:
\[
T_{\text{BubbleSort}}(n) = O(n^2)
\]


\subsection{MergeSort}

MergeSort is another divide-and-conquer algorithm that works by recursively splitting the array in half until each sub-array consists of a single element and then merging those sub-arrays in a manner that results in a sorted array.

\textbf{Complexity:}
- Average Time Complexity: \(O(n \log n)\)
- Worst-Case Time Complexity: \(O(n \log n)\)
- Space Complexity: \(O(n)\)

\textbf{Effective Usage:}
MergeSort is a stable sort and guarantees \(O(n \log n)\) time complexity in the worst case. This makes it a reliable choice for many applications. It is especially useful when data is stored in linked lists because of its linear space complexity, or when stability in sorting is a requirement.  The implementation of the algorithm is represented in Listing \ref{lst:mergesort},


\begin{lstlisting}[language=PseudoCode, label=lst:mergesort, caption=MergeSort and its Merge function]
/**
 * Sorts an array using the MergeSort algorithm.
 * 
 * @param A: The array to be sorted.
 * @param start: The starting index of the portion of A to be sorted.
 * @param end: The ending index of the portion of A to be sorted.
 */
Function MergeSort(A, start, end)
    if start < end
        mid (*@$\leftarrow$@*)  (start + end) / 2
        MergeSort(A, start, mid)
        MergeSort(A, mid + 1, end)
        Merge(A, start, mid, end)
    end if
    
/**
 * Merges two sorted sub-arrays into a single sorted array.
 * 
 * @param A: The main array containing the two sub-arrays.
 * @param start: The starting index of the first sub-array.
 * @param mid: The ending index of the first sub-array and starting index of the second - 1.
 * @param end: The ending index of the second sub-array.
 */
Function Merge(A, start, mid, end)
    n1 (*@$\leftarrow$@*)  mid - start + 1
    n2 (*@$\leftarrow$@*)  end - mid
    let L[0..n1] and R[0..n2] be new arrays
    for i = 0 to n1
        L[i] (*@$\leftarrow$@*)  A[start + i]
    for j (*@$\leftarrow$@*)  0 to n2
        R[j] (*@$\leftarrow$@*)  A[mid + j + 1]
    
    (i, j, k) (*@$\leftarrow$@*) (0, 0, start)
    while i < n1 and j < n2
        if L[i] (*@$\leq$@*) R[j]
            A[k] (*@$\leftarrow$@*)  L[i]
            i (*@$\leftarrow$@*)  i + 1
        else
            A[k] (*@$\leftarrow$@*)  R[j]
            j (*@$\leftarrow$@*)  j + 1
        end if
        k (*@$\leftarrow$@*)  k + 1
    end while
    
    while i < n1
        A[k] (*@$\leftarrow$@*)  L[i]
        i (*@$\leftarrow$@*)  i + 1
        k (*@$\leftarrow$@*)  k + 1
    end while
    while j < n2
        A[k] (*@$\leftarrow$@*)  R[j]
        j (*@$\leftarrow$@*)  j + 1
        k (*@$\leftarrow$@*)  k + 1
    end while
\end{lstlisting}

\subsection{Complexity Analysis of MergeSort}
\subsubsection{Merge Operation}
The complexity of the merge operation is linear, \(O(n)\), because every element is processed once during the merging phase. The main value of the final complexity depends on the \textbf{Merge} function implementation.

\subsubsection{Recursive Splitting}
The sorting operation of MergeSort works by recursively dividing the array into two halves and then merging them. Let \(T(n)\) represent the time complexity of sorting an array of size \(n\). The recurrence relation for MergeSort is given by:

\[
T_{\text{MergeSort}}(n) = 2T_{\text{MergeSort}}\left(\frac{n}{2}\right) + O(n)
\]

Where:
\begin{itemize}
    \item \(2T_{\text{MergeSort}}\left(\frac{n}{2}\right)\) represents the time to sort the two halves.
    \item \(O(n)\) represents the time to merge the two halves.
\end{itemize}

This recurrence can be solved using the Master Theorem or the tree method. For MergeSort, it resolves to:

\[
T_{\text{MergeSort}}(n) = O(n \log n)
\]

Both for the average and worst-case scenarios.


%%%%%%%%%


